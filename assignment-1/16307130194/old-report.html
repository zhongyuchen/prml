<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
<h1 id="assignment-1">Assignment 1</h1>
<p>This assignment is about the exploration of non-parametric methods for density estimation, including histogram method, kernel density estimation and k nearest neighbor method.</p>
<h2 id="overview">0. Overview</h2>
<h3 id="structure">0.1 Structure</h3>
<ul>
<li>source python file
<ul>
<li><code>source.py</code></li>
</ul></li>
<li>solution files
<ul>
<li><code>histogram.py</code>: implementation of histogram method</li>
<li><code>kernel.py</code>: implementation of kernel density estimation</li>
<li><code>knn.py</code>: implementation of k nearest neighbor</li>
</ul></li>
<li>other file
<ul>
<li><code>plot.py</code>: my wrapper of the gm1d plot function for showing gm1d in each subplot</li>
</ul></li>
</ul>
<h3 id="packages">0.2 Packages</h3>
<ul>
<li>Packages including <code>numpy</code>, <code>matplotlib</code>, <code>scipy</code> are used for implementing algorithms.</li>
<li><p>In addition, <code>argparse</code> is used for organizing the program, which can be installed via:</p>
<pre class="commandline"><code>pip install argparse</code></pre></li>
</ul>
<h3 id="usage">0.3 Usage</h3>
<ul>
<li>There are example usages in the following context</li>
<li><p>Show the help message via:</p>
<pre class="commandline"><code>python source.py -h</code></pre></li>
</ul>
<h3 id="parameters">0.4 Parameters</h3>
<ul>
<li><code>n</code>: the amount of sample data</li>
<li><code>b</code>: the amount of bins in histogram method</li>
<li><code>h</code>: the parameter <code>h</code> in Gaussian kernel</li>
<li><code>k</code>: the amount of nearest neighbors</li>
</ul>
<h2 id="histogram-method">1. histogram method</h2>
<h3 id="implementation">1.1 Implementation</h3>
<ul>
<li>This can be easily implemented with <code>hist</code> function in <code>matplotlib.pyplot</code>.</li>
<li><p>Example usage (replace <code>200</code> and <code>25</code> with desired values of <code>n</code> and <code>b</code>):</p>
<pre class="commandline"><code>python source.py --algorithm histogram_method --n 200 --b 25</code></pre></li>
</ul>
<h3 id="vary-num_data">1.2 Vary num_data</h3>
<ul>
<li>By increasing <code>n</code> in the usage above, the result histogram <strong>gets less spiky, captures more underlying features and gets closer to the original distribution</strong>.</li>
<li>As <code>n</code> grows, <code>b</code> can be increased for a certain amount which makes the histogram better without making the histogram spiky.</li>
</ul>
<h3 id="vary-the-number-of-bins">1.3 Vary the number of bins</h3>
<div class="figure">
<img src="pic/histogram.png" alt="histogram" />
<p class="caption">histogram</p>
</div>
<ul>
<li><p>This is the result of <code>N = 200</code> and <code>b = 10 or 25 or 75</code>, which can be obtained with the following command:</p>
<pre class="commandline"><code>python source.py --algorithm histogram_result</code></pre></li>
<li>The effect of <code>b</code>:
<ul>
<li>If <code>b</code> is too small as shown on the left, the histogram is <strong>too smooth</strong> and <strong>captures little details</strong> of the original distribution (orange curve).</li>
<li>If <code>b</code> is too large as shown on the right, the histogram gets <strong>too spiky</strong> with <strong>a lot of structures that are not presented</strong> in the original distribution.</li>
<li>The best <code>b</code> is some intermediate value as shown in the middle, which <strong>captures the details without being spiky</strong>.</li>
</ul></li>
</ul>
<h3 id="find-the-optimal-number-of-bins">1.4 Find the optimal number of bins</h3>
<p>This is achieved with Shimazaki and Shinomoto's choice.</p>
<h4 id="implementaion">Implementaion</h4>
<ul>
<li>generate an array of bin widths</li>
<li>calculate the cost of each bin width with this formula: <br /><span class="math display">$$C(\Delta ) = \frac{2k - v}{\Delta ^{2}}, where\ k\ is\ mean,\ v\ is\ variance\ and\ \Delta\ is\ bin\ width$$</span><br /></li>
<li>The bin width that achieves the lowest cost is the best bin width</li>
</ul>
<h4 id="usage-1">Usage</h4>
<ul>
<li><p>The following usage finds the best <code>b</code> with a given <code>n</code>:</p>
<pre class="commandline"><code>python source.py --algorithm optimal_bin --n 200</code></pre></li>
</ul>
<h4 id="result">Result</h4>
<div class="figure">
<img src="pic/optimal_bin.png" alt="opt_b" />
<p class="caption">opt_b</p>
</div>
<p>If <code>n = 200</code>, <code>b = 25</code> is the best result, which is shown above: + [4, 5, 6, ..., 50] number of bins are tested + on the left, the cost of each bin width is shown in the scatter plot + the histogram with the optimal bin width is shown on the right</p>
<h3 id="other-methods-for-finding-the-optimal-amount">1.5 Other methods for finding the optimal amount</h3>
<h2 id="kernel-density-estimate">2. kernel density estimate</h2>
<p>python source.py --algorithm kernel_density_estimate --n 200 --h 0.35 python source.py --algorithm kde_result python source.py --algorithm optimal_h --n 200</p>
<h2 id="k-nearest-neighbor-method-knn">3. k nearest neighbor method (kNN)</h2>
<p>python source.py --algorithm kNN_kdtree --n 200 --k 14 python source.py --algorithm kNN_result python source.py --algorithm optimal_K --n 200</p>
<p>n sample data rule of thumb: k = sqrt(n)</p>
<p>k large enough to minimize error rate k too small -&gt; noisy decision boundaries</p>
<p>k small enough to only include nearby samples k too large -&gt; over-smoothed boundaries</p>
<p>待查询点M个 样本点N个</p>
<p>L1 distance</p>
<p>V = data[right] - data[left] V = max(data[right] - xi, xi - data[left]) * 2 效果明显好很多</p>
